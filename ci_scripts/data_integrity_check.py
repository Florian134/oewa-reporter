#!/usr/bin/env python3
"""
Data Integrity Check & Repair Tool v1.0
=======================================
PrÃ¼ft und repariert DatenintegritÃ¤t in Airtable:
- Erkennt Duplikate anhand von Unique Keys
- Vergleicht Werte mit Ã–WA API (optional)
- Entfernt Duplikate bei BestÃ¤tigung
- Erstellt Bericht Ã¼ber DatenqualitÃ¤t

WICHTIG: Dieses Script sollte regelmÃ¤ÃŸig ausgefÃ¼hrt werden um DatenintegritÃ¤t sicherzustellen!

Nutzung:
    python ci_scripts/data_integrity_check.py                    # Nur prÃ¼fen
    python ci_scripts/data_integrity_check.py --fix              # Duplikate entfernen
    python ci_scripts/data_integrity_check.py --date 2025-12-20  # Spezifisches Datum
    python ci_scripts/data_integrity_check.py --days 7           # Letzte 7 Tage
    python ci_scripts/data_integrity_check.py --verify-api       # Mit API-Vergleich
"""

import os
import sys
import json
import requests
import argparse
from datetime import date, datetime, timedelta
from collections import defaultdict
from typing import Dict, List, Set, Optional, Tuple

# =============================================================================
# KONFIGURATION
# =============================================================================
INFONLINE_API_KEY = os.environ.get("INFONLINE_API_KEY", "")
AIRTABLE_API_KEY = os.environ.get("AIRTABLE_API_KEY", "")
AIRTABLE_BASE_ID = os.environ.get("AIRTABLE_BASE_ID", "")  # Muss in CI/CD Variables gesetzt sein

# Site-Konfiguration (fÃ¼r API-Vergleich)
SITES = {
    "VOL_Web": {"site_id": "at_w_atvol", "brand": "VOL", "surface": "Web"},
    "VOL_iOS": {"site_id": "at_i_volat", "brand": "VOL", "surface": "iOS"},
    "VOL_Android": {"site_id": "at_a_volat", "brand": "VOL", "surface": "Android"},
    "Vienna_Web": {"site_id": "at_w_atvienna", "brand": "Vienna", "surface": "Web"},
    "Vienna_iOS": {"site_id": "at_i_viennaat", "brand": "Vienna", "surface": "iOS"},
    "Vienna_Android": {"site_id": "at_a_viennaat", "brand": "Vienna", "surface": "Android"},
}

METRICS_MAP = {
    "Page Impressions": {"api": "pageimpressions", "field": "pis"},
    "Visits": {"api": "visits", "field": "visits"},
    "Unique Clients": {"api": "uniqueclients", "field": "uclients"},
}


# =============================================================================
# AIRTABLE FUNKTIONEN
# =============================================================================

def fetch_all_records(start_date: date = None, end_date: date = None) -> List[Dict]:
    """Holt alle Records aus Airtable (optional gefiltert nach Datum)"""
    url = f"https://api.airtable.com/v0/{AIRTABLE_BASE_ID}/Measurements"
    headers = {"Authorization": f"Bearer {AIRTABLE_API_KEY}"}
    
    all_records = []
    offset = None
    
    print("ğŸ“¥ Lade Daten aus Airtable...")
    
    while True:
        params = {"pageSize": 100}
        
        if start_date and end_date:
            # Filter nach Datumsbereich
            formula = f"AND({{Datum}} >= '{start_date.isoformat()}', {{Datum}} <= '{end_date.isoformat()}')"
            params["filterByFormula"] = formula
        elif start_date:
            params["filterByFormula"] = f"{{Datum}} = '{start_date.isoformat()}'"
            
        if offset:
            params["offset"] = offset
            
        try:
            response = requests.get(url, headers=headers, params=params, timeout=30)
            if response.status_code != 200:
                print(f"âŒ Airtable Fehler: {response.status_code}")
                print(response.text[:500])
                break
                
            data = response.json()
            records = data.get("records", [])
            
            for record in records:
                fields = record.get("fields", {})
                all_records.append({
                    "id": record["id"],
                    "datum": fields.get("Datum"),
                    "brand": fields.get("Brand"),
                    "plattform": fields.get("Plattform"),
                    "metrik": fields.get("Metrik"),
                    "wert": fields.get("Wert"),
                    "site_id": fields.get("Site ID"),
                    "unique_key": fields.get("Unique Key"),
                    "erfasst_am": fields.get("Erfasst am"),
                })
            
            offset = data.get("offset")
            if not offset:
                break
                
            print(f"   ... {len(all_records)} Records geladen", end="\r")
            
        except Exception as e:
            print(f"âŒ Fehler: {e}")
            break
    
    print(f"âœ“ {len(all_records)} Records geladen")
    return all_records


def delete_records(record_ids: List[str], dry_run: bool = True) -> int:
    """LÃ¶scht Records aus Airtable"""
    if dry_run:
        print(f"   [DRY-RUN] WÃ¼rde {len(record_ids)} Records lÃ¶schen")
        return len(record_ids)
    
    url = f"https://api.airtable.com/v0/{AIRTABLE_BASE_ID}/Measurements"
    headers = {"Authorization": f"Bearer {AIRTABLE_API_KEY}"}
    
    deleted = 0
    
    # Batch-Delete (max 10 pro Request)
    for i in range(0, len(record_ids), 10):
        batch = record_ids[i:i+10]
        
        try:
            # Airtable Delete erwartet records[] Parameter
            params = "&".join([f"records[]={rid}" for rid in batch])
            response = requests.delete(f"{url}?{params}", headers=headers, timeout=30)
            
            if response.status_code == 200:
                deleted += len(batch)
            else:
                print(f"   âš ï¸ Batch-Fehler: {response.status_code}")
                
        except Exception as e:
            print(f"   âŒ Fehler beim LÃ¶schen: {e}")
    
    return deleted


# =============================================================================
# API FUNKTIONEN
# =============================================================================

def fetch_api_value(site_id: str, metric_api: str, target_date: date) -> Optional[int]:
    """Holt den Wert direkt von der INFOnline API"""
    if not INFONLINE_API_KEY:
        return None
        
    url = f"https://reportingapi.infonline.de/api/v1/{metric_api}"
    params = {
        "site": site_id,
        "date": target_date.isoformat(),
        "aggregation": "DAY"
    }
    headers = {
        "authorization": INFONLINE_API_KEY,
        "Accept": "application/json"
    }
    
    try:
        response = requests.get(url, params=params, headers=headers, timeout=30)
        if response.status_code == 200:
            data = response.json()
            if "data" in data and "iom" in data["data"] and len(data["data"]["iom"]) > 0:
                field = METRICS_MAP.get(metric_api, {}).get("field", metric_api)
                # FÃ¼r API-Metrik den Feldnamen finden
                for m, info in METRICS_MAP.items():
                    if info["api"] == metric_api:
                        field = info["field"]
                        break
                return data["data"]["iom"][0].get(field)
    except:
        pass
    
    return None


# =============================================================================
# ANALYSE FUNKTIONEN
# =============================================================================

def find_duplicates(records: List[Dict]) -> Dict[str, List[Dict]]:
    """Findet Duplikate anhand des Unique Key"""
    key_records = defaultdict(list)
    
    for record in records:
        key = record.get("unique_key")
        if key:
            key_records[key].append(record)
    
    # Nur Duplikate (mehr als 1 Record pro Key)
    duplicates = {k: v for k, v in key_records.items() if len(v) > 1}
    
    return duplicates


def analyze_data_quality(records: List[Dict]) -> Dict:
    """Analysiert DatenqualitÃ¤t und erstellt Bericht"""
    analysis = {
        "total_records": len(records),
        "unique_keys": set(),
        "dates": set(),
        "brands": set(),
        "platforms": set(),
        "metrics": set(),
        "duplicates": {},
        "missing_unique_key": 0,
        "issues": []
    }
    
    for record in records:
        uk = record.get("unique_key")
        if uk:
            analysis["unique_keys"].add(uk)
        else:
            analysis["missing_unique_key"] += 1
            analysis["issues"].append(f"Missing Unique Key: {record['id']}")
        
        if record.get("datum"):
            analysis["dates"].add(record["datum"])
        if record.get("brand"):
            analysis["brands"].add(record["brand"])
        if record.get("plattform"):
            analysis["platforms"].add(record["plattform"])
        if record.get("metrik"):
            analysis["metrics"].add(record["metrik"])
    
    # Duplikate finden
    analysis["duplicates"] = find_duplicates(records)
    
    return analysis


def compare_with_api(records: List[Dict], date_to_check: date) -> List[Dict]:
    """Vergleicht Airtable-Werte mit API-Werten"""
    if not INFONLINE_API_KEY:
        print("âš ï¸ INFONLINE_API_KEY nicht gesetzt - API-Vergleich Ã¼bersprungen")
        return []
    
    print(f"\nğŸ” Vergleiche mit Ã–WA API fÃ¼r {date_to_check.isoformat()}...")
    
    discrepancies = []
    
    # Aggregiere Airtable-Werte pro Unique Key
    airtable_values = {}
    for record in records:
        if record.get("datum") == date_to_check.isoformat():
            key = record.get("unique_key")
            if key and "_MONTH_" not in key:  # Nur Tagesdaten
                if key not in airtable_values:
                    airtable_values[key] = {"total": 0, "count": 0, "records": []}
                airtable_values[key]["total"] += record.get("wert", 0) or 0
                airtable_values[key]["count"] += 1
                airtable_values[key]["records"].append(record)
    
    # Vergleiche mit API
    for site_name, site_info in SITES.items():
        for metric_name, metric_info in METRICS_MAP.items():
            # Unique Key Format: {Datum}_{Brand}_{Surface}_{Metrik}
            unique_key = f"{date_to_check.isoformat()}_{site_info['brand']}_{site_info['surface']}_{metric_name}"
            
            if unique_key in airtable_values:
                airtable_total = airtable_values[unique_key]["total"]
                record_count = airtable_values[unique_key]["count"]
                
                # API-Wert holen
                api_value = fetch_api_value(site_info["site_id"], metric_info["api"], date_to_check)
                
                if api_value is not None:
                    # Vergleich: Erwarte dass Airtable-Summe == API-Wert
                    # Bei Duplikaten: Airtable-Summe = API-Wert * Anzahl Duplikate
                    expected_api = api_value
                    diff_pct = ((airtable_total - expected_api) / expected_api * 100) if expected_api > 0 else 0
                    
                    if abs(diff_pct) > 1 or record_count > 1:  # Mehr als 1% Abweichung oder Duplikate
                        discrepancies.append({
                            "unique_key": unique_key,
                            "brand": site_info["brand"],
                            "surface": site_info["surface"],
                            "metric": metric_name,
                            "airtable_total": airtable_total,
                            "airtable_count": record_count,
                            "api_value": api_value,
                            "diff_pct": diff_pct,
                            "is_duplicate": record_count > 1,
                            "records": airtable_values[unique_key]["records"]
                        })
                        
                        print(f"   {'âŒ DUPLIKAT' if record_count > 1 else 'âš ï¸'} {unique_key}")
                        print(f"      Airtable: {airtable_total:,} ({record_count}x)")
                        print(f"      API:      {api_value:,}")
                        print(f"      Diff:     {diff_pct:+.1f}%")
    
    return discrepancies


# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Ã–WA Data Integrity Check & Repair Tool",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument("--date", type=str, help="Spezifisches Datum prÃ¼fen (YYYY-MM-DD)")
    parser.add_argument("--days", type=int, default=7, help="Letzte X Tage prÃ¼fen (default: 7)")
    parser.add_argument("--fix", action="store_true", help="Duplikate automatisch entfernen")
    parser.add_argument("--verify-api", action="store_true", help="Mit Ã–WA API vergleichen")
    parser.add_argument("--dry-run", action="store_true", help="Nur simulieren, nichts Ã¤ndern")
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ğŸ” Ã–WA DATA INTEGRITY CHECK v1.0")
    print("=" * 70)
    
    # API Keys prÃ¼fen
    if not AIRTABLE_API_KEY:
        print("âŒ AIRTABLE_API_KEY nicht gesetzt!")
        sys.exit(1)
    
    # Datumsbereiche
    if args.date:
        start_date = date.fromisoformat(args.date)
        end_date = start_date
    else:
        end_date = date.today()
        start_date = end_date - timedelta(days=args.days)
    
    print(f"\nğŸ“… PrÃ¼fe Zeitraum: {start_date.isoformat()} bis {end_date.isoformat()}")
    
    # Daten laden
    records = fetch_all_records(start_date, end_date)
    
    if not records:
        print("âœ“ Keine Daten fÃ¼r den Zeitraum gefunden.")
        return
    
    # Analyse durchfÃ¼hren
    print("\n" + "=" * 70)
    print("ğŸ“Š ANALYSE")
    print("=" * 70)
    
    analysis = analyze_data_quality(records)
    
    print(f"\nğŸ“ˆ STATISTIKEN:")
    print(f"   Total Records:      {analysis['total_records']}")
    print(f"   Unique Keys:        {len(analysis['unique_keys'])}")
    print(f"   Daten ohne Key:     {analysis['missing_unique_key']}")
    print(f"   Duplikate gefunden: {len(analysis['duplicates'])}")
    print(f"   Betroffene Tage:    {len(analysis['dates'])}")
    print(f"   Brands:             {', '.join(sorted(analysis['brands']))}")
    print(f"   Plattformen:        {', '.join(sorted(analysis['platforms']))}")
    print(f"   Metriken:           {', '.join(sorted(analysis['metrics']))}")
    
    # Duplikate anzeigen
    if analysis["duplicates"]:
        print("\n" + "=" * 70)
        print("âš ï¸  DUPLIKATE GEFUNDEN!")
        print("=" * 70)
        
        total_duplicate_records = 0
        records_to_delete = []
        
        for unique_key, dup_records in sorted(analysis["duplicates"].items()):
            count = len(dup_records)
            total_duplicate_records += count - 1  # -1 weil wir einen behalten wollen
            
            print(f"\nğŸ”‘ {unique_key}")
            print(f"   Anzahl: {count} Records (sollte 1 sein)")
            
            # Sortiere nach Erfassungsdatum (behalte Ã¤ltesten)
            sorted_records = sorted(dup_records, key=lambda x: x.get("erfasst_am") or "")
            
            for i, rec in enumerate(sorted_records):
                wert = rec.get("wert") or 0
                erfasst = rec.get("erfasst_am", "")[:19] if rec.get("erfasst_am") else "?"
                marker = "âœ… BEHALTEN" if i == 0 else "âŒ LÃ–SCHEN"
                print(f"   {marker} | ID: {rec['id'][:8]}... | Wert: {wert:,} | Erfasst: {erfasst}")
                
                # Alle auÃŸer dem ersten (Ã¤ltesten) zum LÃ¶schen markieren
                if i > 0:
                    records_to_delete.append(rec["id"])
        
        print(f"\nğŸ“‹ ZUSAMMENFASSUNG:")
        print(f"   Duplikate-Gruppen:     {len(analysis['duplicates'])}")
        print(f"   Records zum LÃ¶schen:   {len(records_to_delete)}")
        
        # API-Vergleich (wenn aktiviert)
        if args.verify_api:
            discrepancies = []
            current = start_date
            while current <= end_date:
                disc = compare_with_api(records, current)
                discrepancies.extend(disc)
                current += timedelta(days=1)
            
            if discrepancies:
                print(f"\nâš ï¸ {len(discrepancies)} Abweichungen zur Ã–WA API gefunden!")
        
        # Fix durchfÃ¼hren (wenn --fix angegeben)
        if args.fix and records_to_delete:
            print("\n" + "=" * 70)
            print("ğŸ”§ REPARATUR")
            print("=" * 70)
            
            if args.dry_run or not args.fix:
                print(f"\n[DRY-RUN] WÃ¼rde {len(records_to_delete)} Duplikat-Records lÃ¶schen:")
                for rid in records_to_delete[:5]:
                    print(f"   â€¢ {rid}")
                if len(records_to_delete) > 5:
                    print(f"   ... und {len(records_to_delete) - 5} weitere")
            else:
                print(f"\nğŸ—‘ï¸ LÃ¶sche {len(records_to_delete)} Duplikat-Records...")
                deleted = delete_records(records_to_delete, dry_run=False)
                print(f"âœ“ {deleted} Records gelÃ¶scht")
        
        elif records_to_delete:
            print("\nğŸ’¡ Tipp: FÃ¼hre mit --fix aus, um Duplikate zu entfernen:")
            print(f"   python ci_scripts/data_integrity_check.py --date {start_date.isoformat()} --fix")
    
    else:
        print("\nâœ… KEINE DUPLIKATE GEFUNDEN")
        
        # Optional: API-Vergleich auch ohne Duplikate
        if args.verify_api:
            print("\nğŸ” FÃ¼hre API-Vergleich durch...")
            current = start_date
            while current <= end_date:
                compare_with_api(records, current)
                current += timedelta(days=1)
    
    print("\n" + "=" * 70)
    print("âœ“ INTEGRITY CHECK ABGESCHLOSSEN")
    print("=" * 70)


if __name__ == "__main__":
    main()

